{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport copy\nimport time\nimport random\nimport string\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# Utils\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\n# For Transformer Models\n\nos.system('pip uninstall -y transformers')\nos.system('pip uninstall -y tokenizers')\nos.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels transformers')\nos.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels tokenizers')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\n#from transformers import AutoTokenizer, AutoModel, AutoConfig\n#from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW, get_scheduler, get_linear_schedule_with_warmup, get_constant_schedule_with_warmup\nimport matplotlib.pyplot as plt\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\n%env TOKENIZERS_PARALLELISM=true","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:45:06.223838Z","iopub.execute_input":"2022-09-07T06:45:06.224412Z","iopub.status.idle":"2022-09-07T06:45:36.114271Z","shell.execute_reply.started":"2022-09-07T06:45:06.224377Z","shell.execute_reply":"2022-09-07T06:45:36.11313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/feedback-prize-english-language-learning/train.csv')\ny = df[['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']]","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:45:36.11611Z","iopub.execute_input":"2022-09-07T06:45:36.11667Z","iopub.status.idle":"2022-09-07T06:45:36.302131Z","shell.execute_reply.started":"2022-09-07T06:45:36.11663Z","shell.execute_reply":"2022-09-07T06:45:36.301146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\"seed\": 2021,\n          \"epochs\": 5,\n          \"model_name\": \"microsoft/deberta-v3-base\",\n          \"train_batch_size\": 8,\n          \"valid_batch_size\": 8,\n          \"max_length\": 512,\n          \"learning_rate\": 7e-6,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"gradient_checkpointing\" : True,\n          \"min_lr\": 1e-6,\n          \"T_max\": 500,\n          \"weight_decay\": 1e-6,\n          \"n_fold\": 5,\n          \"n_accumulate\": 1,\n          \"num_classes\": 1,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n          }\n\n\n\n#set notebook SEED\ndef set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:36:21.142682Z","iopub.status.idle":"2022-09-07T06:36:21.143441Z","shell.execute_reply.started":"2022-09-07T06:36:21.143191Z","shell.execute_reply":"2022-09-07T06:36:21.143215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----- TOKENIZER -----\nspecial_tokens_list = []\nspecial_tokens_list.append('[BR]')\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\nCONFIG[\"tokenizer\"].save_pretrained('./tokenizer/')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = KFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n\nfor fold, ( _, val_) in enumerate(skf.split(X=df, y=y)):\n    df.loc[val_ , \"kfold\"] = int(fold)\n    \ndf[\"kfold\"] = df[\"kfold\"].astype(int)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:36:21.144771Z","iopub.status.idle":"2022-09-07T06:36:21.145513Z","shell.execute_reply.started":"2022-09-07T06:36:21.14526Z","shell.execute_reply":"2022-09-07T06:36:21.145283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get Kfold score distribution\nfor fold in df[\"kfold\"].unique():\n  #fig = plt.figure()\n  df[df[\"kfold\"] == fold]['conventions'].hist(bins = 30,  histtype=u'step')\n  #print(df[df[\"kfold\"] == fold]['y'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:36:21.146864Z","iopub.status.idle":"2022-09-07T06:36:21.147589Z","shell.execute_reply.started":"2022-09-07T06:36:21.147358Z","shell.execute_reply":"2022-09-07T06:36:21.14738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\n#extend Dataset class\nclass FPDataset(torch.utils.data.Dataset):\n   \n    def __init__(self, texts, targets, tokenizer, seq_len=CONFIG['max_length']):        \n        self.texts = texts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.seq_len = seq_len\n    \n    def __len__(self):\n        \"\"\"Returns the length of dataset.\"\"\"\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])         \n        tokenized = self.tokenizer(\n            text,            \n            max_length = self.seq_len,                                \n            padding = \"max_length\",     # Pad to the specified max_length. \n            truncation = True,          # Truncate to the specified max_length. \n            add_special_tokens = True,  # Whether to insert [CLS], [SEP], <s>, etc.   \n            return_attention_mask = True            \n        )         \n        return {\"ids\": torch.tensor(tokenized[\"input_ids\"], dtype=torch.long),\n                \"masks\": torch.tensor(tokenized[\"attention_mask\"], dtype=torch.long),\n                \"target\": torch.tensor(self.targets[idx], dtype=torch.float)\n               }\n    \n#define model \nclass FPModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg['model_name'], output_hidden_states=True)\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n        else:\n            self.config = torch.load(config_path)\n            \n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg['model_name'], config=self.config)\n        else:\n            self.model = AutoModel(self.config)\n        if self.cfg['gradient_checkpointing']:\n            self.model.gradient_checkpointing_enable()\n            \n        self.pool = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, 6)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n        \n    def feature(self, input_ids, attention_mask):\n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_states = outputs[0]\n        feature = self.pool(last_hidden_states, attention_mask)\n        return feature\n\n    def forward(self, input_ids, attention_mask):\n        feature = self.feature(input_ids, attention_mask)\n        output = self.fc(feature)\n        return output\n    \ndef loss_fn(predictions, targets):\n    return torch.sqrt(nn.MSELoss()(predictions, targets))\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:21:30.780561Z","iopub.execute_input":"2022-09-06T13:21:30.780919Z","iopub.status.idle":"2022-09-06T13:21:30.801841Z","shell.execute_reply.started":"2022-09-06T13:21:30.780888Z","shell.execute_reply":"2022-09-06T13:21:30.800901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, scheduler):    \n    \n    model.train()                                # Put the model in training mode.              \n    dataset_size = 0\n    running_loss = 0.0\n\n    lr_list = []\n    train_losses = []         \n    \n    bar = tqdm(enumerate(data_loader), total=len(data_loader))\n\n    for step, batch in bar:\n        \n        ids = batch[\"ids\"].to(device, dtype=torch.long)\n        masks = batch[\"masks\"].to(device, dtype=torch.long)\n        targets = batch[\"target\"].to(device, dtype=torch.float) \n        \n        optimizer.zero_grad()                    # To zero out the gradients.\n\n        outputs = model(ids, masks).squeeze()  # Predictions from 1 batch of data.\n        \n        #print(outputs, targets)\n        \n        loss = loss_fn(outputs, targets)   \n        #print(outputs.size(), targets.size(), loss.item())# Get the training loss.\n        train_losses.append(loss.item())\n        \n        loss = loss / CONFIG['n_accumulate']\n        loss.backward()\n    \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            optimizer.step()\n            lr_list.append(optimizer.param_groups[0][\"lr\"])\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n\n        #get progress bar\n        batch_size = ids.size(0)\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n        \n    return train_losses, lr_list","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:21:32.022131Z","iopub.execute_input":"2022-09-06T13:21:32.023076Z","iopub.status.idle":"2022-09-06T13:21:32.034249Z","shell.execute_reply.started":"2022-09-06T13:21:32.023041Z","shell.execute_reply":"2022-09-06T13:21:32.033019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef validate_fn(data_loader, model, device):\n        \n    model.eval()                                    # Put model in evaluation mode.\n    dataset_size = 0\n    running_loss = 0.0\n\n    val_losses = []\n        \n    with torch.no_grad():                           # Disable gradient calculation.\n        \n        bar = tqdm(enumerate(data_loader), total=len(data_loader))\n\n        for step, batch in bar:                  # Loop over all batches.\n            \n            ids = batch[\"ids\"].to(device, dtype=torch.long)\n            masks = batch[\"masks\"].to(device, dtype=torch.long)\n            targets = batch[\"target\"].to(device, dtype=torch.float)\n\n            outputs = model(ids, masks).squeeze() # Predictions from 1 batch of data.\n            \n            loss = loss_fn(outputs, targets)        # Get the validation loss.\n            val_losses.append(loss.item())\n\n            #get progress bar\n            batch_size = ids.size(0)\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n            \n            epoch_loss = running_loss / dataset_size\n            \n            bar.set_postfix(Train_Loss=epoch_loss)\n            \n        gc.collect()\n            \n    return val_losses ","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:21:32.542692Z","iopub.execute_input":"2022-09-06T13:21:32.543195Z","iopub.status.idle":"2022-09-06T13:21:32.557511Z","shell.execute_reply.started":"2022-09-06T13:21:32.543162Z","shell.execute_reply":"2022-09-06T13:21:32.556265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot LR and Loss functions\ndef plot_lr_schedule(learning_rates):\n    x = np.arange(len(learning_rates))\n    plt.plot(x, learning_rates)\n    plt.title(f'Linear schedule')\n    plt.ylabel(\"Learning Rate\")\n    plt.xlabel(\"Training Steps\")\n    plt.show()\n    \ndef plot_train_val_losses(train_losses, val_losses, fold):\n    x = np.arange(len(train_losses))\n    plt.plot(x, train_losses, label=\"training loss\", marker='o')\n    plt.plot(x, val_losses, label=\"validation loss\", marker='o')\n    plt.legend(loc=\"best\")   # to show the labels.\n    plt.title(f'Fold {fold}: model')    \n    plt.ylabel(\"RMSE\")\n    plt.xlabel(f\"Epoch\")    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:21:32.901627Z","iopub.execute_input":"2022-09-06T13:21:32.902237Z","iopub.status.idle":"2022-09-06T13:21:32.914182Z","shell.execute_reply.started":"2022-09-06T13:21:32.902166Z","shell.execute_reply":"2022-09-06T13:21:32.913252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_training(df):  \n    \n    \"\"\"\n    run training\n    \"\"\"    \n    EPOCHS = CONFIG['epochs']  \n    FOLDS = [0,1,2,3,4]   # Set the list of folds you want to train\n    EARLY_STOP_THRESHOLD = 3    \n    TRAIN_BS = CONFIG['train_batch_size']             # Training batch size     \n    VAL_BS = CONFIG['valid_batch_size']               # Validation batch size  \n    cv = []                   # A list to hold the cross validation scores\n    \n    #=========================================================================\n    # Prepare data and model for training\n    #=========================================================================\n    \n    for fold in FOLDS:\n        \n        # Initialize the tokenizer\n        tokenizer =  AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\n        # Fetch training data\n        df_train = df[df[\"kfold\"] != fold].reset_index(drop=True)\n\n        # Fetch validation data\n        df_val = df[df[\"kfold\"] == fold].reset_index(drop=True)\n\n        # Initialize training dataset\n        train_dataset = FPDataset(df_train['full_text'].values, df_train[['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']].values, tokenizer)\n\n        # Initialize validation dataset\n        val_dataset = FPDataset(df_val['full_text'].values, df_val[['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']].values, tokenizer)\n\n        # Create training dataloader\n        train_data_loader = DataLoader(train_dataset, batch_size = TRAIN_BS,\n                                       shuffle = True, num_workers = 2)\n\n        # Create validation dataloader\n        val_data_loader = DataLoader(val_dataset, batch_size = VAL_BS,\n                                     shuffle = False, num_workers = 2)\n\n        # Initialize the cuda device (or use CPU if you don't have GPU)\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        # Load model and send it to the device.        \n        model =  FPModel(CONFIG, config_path=None, pretrained=True)\n        torch.save(model.config, './config.pth')\n        model = model.to(device)\n        \n\n        # Get the AdamW optimizer\n        optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n\n        # Calculate the number of training steps (this is used by scheduler).\n        # training steps = [number of batches] x [number of epochs].\n        train_steps = int(len(df_train) / TRAIN_BS * EPOCHS)    \n\n        # Get the learning rate scheduler    \n        scheduler = get_linear_schedule_with_warmup(                \n                optimizer = optimizer,\n                num_warmup_steps = int(train_steps*0.1),\n                num_training_steps = train_steps\n        )\n\n        #=========================================================================\n        # Training Loop - Start training the epochs\n        #=========================================================================\n\n        print(f\"===== FOLD: {fold} =====\")    \n        best_rmse = 999\n        early_stopping_counter = 0       \n        all_train_losses = []\n        all_val_losses = []\n        all_lr = []\n\n        for epoch in range(EPOCHS):\n\n            # Call the train function and get the training loss\n            train_losses, lr_list = train_fn(train_data_loader, model, optimizer, device, scheduler)\n            train_loss = np.mean(train_losses)   \n            all_train_losses.append(train_loss)\n            all_lr.extend(lr_list)\n\n            # Perform validation and get the validation loss\n            val_losses = validate_fn(val_data_loader, model, device)\n            val_loss = np.mean(val_losses)\n            all_val_losses.append(val_loss)    \n\n            rmse = val_loss\n\n            # If there's improvement on the validation loss, save the model checkpoint.\n            # Else do early stopping if threshold is reached.\n            if rmse < best_rmse:            \n                torch.save(model.state_dict(), f\"model_base_LR_fold_{fold}.bin\")\n                print(f\"FOLD: {fold}, Epoch: {epoch}, RMSE = {round(rmse,4)}, checkpoint saved.\")\n                best_rmse = rmse\n                early_stopping_counter = 0\n            else:\n                print(f\"FOLD: {fold}, Epoch: {epoch}, RMSE = {round(rmse,4)}\")\n                early_stopping_counter += 1\n            if early_stopping_counter > EARLY_STOP_THRESHOLD:\n                print(f\"FOLD: {fold}, Epoch: {epoch}, RMSE = {round(rmse,4)}\")\n                print(f\"Early stopping triggered! Best RMSE: {round(best_rmse,4)}\\n\")                \n                break\n\n        # Plot the losses and learning rate schedule.\n        plot_train_val_losses(all_train_losses, all_val_losses, fold)\n        plot_lr_schedule(all_lr)   \n        \n        # Keep the best_rmse as cross validation score for the fold.\n        cv.append(best_rmse)\n        \n    # Print the cross validation scores and their average.\n    cv_rounded = [ round(elem, 4) for elem in cv ] \n    print(f\"CV: {cv_rounded}\") \n    print(f\"Average CV: {round(np.mean(cv), 4)}\\n\") \n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:23:52.139051Z","iopub.execute_input":"2022-09-06T13:23:52.139884Z","iopub.status.idle":"2022-09-06T13:23:52.169379Z","shell.execute_reply.started":"2022-09-06T13:23:52.139848Z","shell.execute_reply":"2022-09-06T13:23:52.168352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_training(df)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:23:53.343052Z","iopub.execute_input":"2022-09-06T13:23:53.343654Z","iopub.status.idle":"2022-09-06T13:40:01.395087Z","shell.execute_reply.started":"2022-09-06T13:23:53.343616Z","shell.execute_reply":"2022-09-06T13:40:01.393433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}